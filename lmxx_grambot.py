import os
import logging
import datetime
import pytz
from openai import OpenAI
from dotenv import load_dotenv
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_API_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CHAT_ID = os.getenv("CHAT_ID")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=OPENAI_API_KEY)

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone("Asia/Seoul")

# ë¡œê·¸ ë””ë ‰í„°ë¦¬ ì„¤ì •
LOG_DIR = "word_logs"
os.makedirs(LOG_DIR, exist_ok=True)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)

# GPT APIë¥¼ í†µí•´ ë‹¨ì–´ ëª©ë¡ ìƒì„± ë° ë¡œê·¸ ì €ì¥
def generate_word_list():
    prompt = (
        "Generate a list of 50 English words suitable for university undergraduate and conversation level with their Korean translations. "
        "Output should be in the format 'English word - Korean meaning' with one word per line. "
        "No additional explanation, just the word list."
    )    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=500
        )
        word_list = response.choices[0].message.content.strip()

        # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ë°˜ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        today = datetime.datetime.now(KST).strftime("%Y-%m-%d")
        file_path = os.path.join(LOG_DIR, f"{today}_words.txt")

        # ë‹¨ì–´ ëª©ë¡ ë¡œê·¸ íŒŒì¼ì— ì €ì¥
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(word_list)

        logging.info(f"ë‹¨ì–´ ëª©ë¡ì´ {file_path}ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return word_list

    except Exception as e:
        logging.error(f"OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "âš ï¸ ë‹¨ì–´ ëª©ë¡ì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ì´ì „ ë‹¨ì–´ ë³‘í•©
def get_previous_words():
    files = sorted(f for f in os.listdir(LOG_DIR) if f.endswith("_words.txt"))
    merged_text = []

    for file in files:
        file_path = os.path.join(LOG_DIR, file)
        with open(file_path, "r", encoding="utf-8") as f:
            date = file.split("_")[0]
            merged_text.append(f"\nğŸ“… {date} ë‹¨ì–´ ëª©ë¡:\n" + f.read())

    return "\n".join(merged_text) if merged_text else "ì´ì „ ë‹¨ì–´ ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤."

# /start ëª…ë ¹ì–´ ì²˜ë¦¬
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("ì•ˆë…•í•˜ì„¸ìš”! /words ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# /words ëª…ë ¹ì–´ë¡œ ë‹¨ì–´ ëª©ë¡ ì œê³µ
async def words(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("GPT APIë¥¼ í†µí•´ ë‹¨ì–´ ëª©ë¡ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
    word_list = generate_word_list()
    previous_words = get_previous_words()

    await update.message.reply_text(f"ğŸ“š ì˜¤ëŠ˜ì˜ ë‹¨ì–´ ëª©ë¡:\n\n{word_list} \n\n ì´ì „ ë‹¨ì–´ ëª©ë¡:\n{previous_words}")

# ë§¤ì¼ ë‹¨ì–´ ìë™ ì „ì†¡
async def daily_word(context: ContextTypes.DEFAULT_TYPE):
    word_list = generate_word_list()
    previous_words = get_previous_words()

    message = f"ğŸ“š ì˜¤ëŠ˜ì˜ ë‹¨ì–´ ëª©ë¡:\n\n{word_list}\n\nğŸ“– ì´ì „ ë‹¨ì–´ ëª©ë¡:\n{previous_words}"
    await context.bot.send_message(chat_id=CHAT_ID, text=message)

# JobQueue ì„¤ì •
async def post_init(application):
    job_queue = application.job_queue
    kst_time = datetime.time(hour=9, minute=0, second=0, tzinfo=KST)
    job_queue.run_daily(daily_word, time=kst_time)

# ë©”ì¸ í•¨ìˆ˜
def main():
    app = ApplicationBuilder().token(TELEGRAM_TOKEN).post_init(post_init).build()

    # ëª…ë ¹ì–´ í•¸ë“¤ëŸ¬ ë“±ë¡
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("words", words))

    # ë´‡ ì‹¤í–‰
    app.run_polling()

if __name__ == '__main__':
    main()
